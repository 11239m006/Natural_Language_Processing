{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOWOs6Ty57EE0K/7sP9h47e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11239m006/Natural_Language_Processing/blob/main/Untitled10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9mkZ_40k6JKP",
        "outputId": "975c6e75-4bc3-4db8-da6f-df0e8a3397db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lemmas: ['The', 'child', 'were', 'running', 'happily', '.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "sentence = \"The children were running happily.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "print(\"Lemmas:\", lemmas)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plural(noun):\n",
        "    if noun.endswith(\"y\"):\n",
        "        return noun[:-1] + \"ies\"\n",
        "    elif noun.endswith((\"s\", \"x\", \"z\", \"ch\", \"sh\")):\n",
        "        return noun + \"es\"\n",
        "    else:\n",
        "        return noun + \"s\"\n",
        "\n",
        "def past_tense(verb):\n",
        "    if verb.endswith(\"e\"):\n",
        "        return verb + \"d\"\n",
        "    elif verb.endswith(\"y\"):\n",
        "        return verb[:-1] + \"ied\"\n",
        "    else:\n",
        "        return verb + \"ed\"\n",
        "\n",
        "words = [\"study\", \"play\", \"fix\"]\n",
        "for w in words:\n",
        "    print(w, \"→\", plural(w), \",\", past_tense(w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHWmZK44-1Sr",
        "outputId": "a4f5a9b1-2f99-405f-8cec-c2a0e7f26735"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study → studies , studied\n",
            "play → plaies , plaied\n",
            "fix → fixes , fixed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "print(lemmatizer.lemmatize(\"studies\", \"v\"))\n",
        "print(lemmatizer.lemmatize(\"studied\", \"v\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJdhGk1T-40t",
        "outputId": "31115c34-12f2-4e2c-e0ab-1cbeca8d7509"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "study\n",
            "study\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk import FreqDist\n",
        "\n",
        "text = \"I like pizza and I like pasta\"\n",
        "tokens = word_tokenize(text)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "\n",
        "bigram_freq = FreqDist(bigrams)\n",
        "word_freq = FreqDist(tokens)\n",
        "V = len(set(tokens))\n",
        "\n",
        "def laplace_prob(w1, w2):\n",
        "    return (bigram_freq[(w1, w2)] + 1) / (word_freq[w1] + V)\n",
        "\n",
        "def beforelaplace(w1, w2):\n",
        "    return (bigram_freq[(w1, w2)]) / (word_freq[w1] + V)\n",
        "\n",
        "\n",
        "print(\"Before:\", beforelaplace(\"like\",\"dogs\"))\n",
        "print(\"After smoothing:\", laplace_prob(\"like\",\"dogs\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "INbzzG_Xg6QZ",
        "outputId": "7d8873a1-c49a-452a-e6ca-d0a62ee50e84"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before: 0.0\n",
            "After smoothing: 0.14285714285714285\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prefixes = [\"un\", \"re\", \"in\", \"im\", \"dis\"]\n",
        "suffixes = [\"ing\", \"er\", \"ed\", \"ly\", \"ment\", \"ness\", \"ful\"]\n",
        "def morph(w):\n",
        "  p=\"\"\n",
        "  s=\"\"\n",
        "  root=w\n",
        "  for i in prefixes:\n",
        "    if w.startswith(i):\n",
        "      p=i\n",
        "      root=w[len(i):]\n",
        "  for j in suffixes:\n",
        "    if root.endswith(j):\n",
        "       s=j\n",
        "       root=root[:-len(j)]\n",
        "  return p,root,s\n",
        "\n",
        "\n",
        "\n",
        "words=[\"teacher\",\"unhappy\",\"Management\",\"Beautiful\",\"kindness\",\"impossible\",\"reinforcement\"]\n",
        "for w in words:\n",
        "  print(w,\"->\",morph(w))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UR6rpJ30kuCl",
        "outputId": "cab54d3a-7448-40e3-c422-91f828889edb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "teacher -> ('', 'teach', 'er')\n",
            "unhappy -> ('un', 'happy', '')\n",
            "Management -> ('', 'Manage', 'ment')\n",
            "Beautiful -> ('', 'Beauti', 'ful')\n",
            "kindness -> ('', 'kind', 'ness')\n",
            "impossible -> ('im', 'possible', '')\n",
            "reinforcement -> ('re', 'inforce', 'ment')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ud-o-ScNqamE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "text=\"madhumitha is amazing and talented!\"\n",
        "token=word_tokenize(text)\n",
        "bigram=list(ngrams(token,2))\n",
        "trigram=list(ngrams(token,3))\n",
        "bigram_freq=FreqDist(bigram)\n",
        "trigram_freq=FreqDist(trigram)\n",
        "print(\"bigrams:\",bigram)\n",
        "print(\"trigram:\",trigram)\n",
        "print(\"Bigram_freq:\",bigram_freq)\n",
        "print(\"trigram_freq:\",trigram_freq)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jpFyVSLEmCOg",
        "outputId": "598b38cb-39bf-4df6-91b5-7ddadbd0812d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bigrams: [('madhumitha', 'is'), ('is', 'amazing'), ('amazing', 'and'), ('and', 'talented'), ('talented', '!')]\n",
            "trigram: [('madhumitha', 'is', 'amazing'), ('is', 'amazing', 'and'), ('amazing', 'and', 'talented'), ('and', 'talented', '!')]\n",
            "Bigram_freq: <FreqDist with 5 samples and 5 outcomes>\n",
            "trigram_freq: <FreqDist with 4 samples and 4 outcomes>\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.util import ngrams\n",
        "text=\"madhumitha is amazing and talented!\"\n",
        "token=word_tokenize(text)\n",
        "bigram=list(ngrams(token,2))\n",
        "\n",
        "bigram_freq=FreqDist(bigram)\n",
        "word_freq=FreqDist(token)\n",
        "v=len(set(token))\n",
        "def laplace(w1,w2):\n",
        "  return (bigram_freq[(w1,w2)]+1)/(word_freq[w1]+v)\n",
        "print(\"smoothing:\",laplace(\"like\",\"dog\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QAiPR4J9qnQC",
        "outputId": "ebba7dbb-1d56-4f5d-c709-d927de6826a7"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "smoothing: 0.16666666666666666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('brown')\n",
        "nltk.download('universal_tagset')\n",
        "from nltk.tag import hmm\n",
        "from nltk.corpus import brown\n",
        "\n",
        "trainner=hmm.HiddenMarkovModelTrainer()\n",
        "\n",
        "traindata=brown.tagged_sents(tagset='universal')[:3000]\n",
        "\n",
        "tagger=trainner.train_supervised(traindata)\n",
        "print(tagger.tag([\"she\",\"said\",\"beautiful\"]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyHYiFnQrwkt",
        "outputId": "9ff501ba-772b-4246-d771-2dfe18041621"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Package universal_tagset is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('she', 'PRON'), ('said', 'VERB'), ('beautiful', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. pos tagger"
      ],
      "metadata": {
        "id": "x3lm_MVh650e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk import pos_tag, word_tokenize\n",
        "\n",
        "text = \"John is playing football\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\nPOS Tags:\", pos_tag(tokens))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SF2KUAxk678k",
        "outputId": "1ff0a702-c1a4-4ad0-943c-98ec0dc3f238"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tags: [('John', 'NNP'), ('is', 'VBZ'), ('playing', 'VBG'), ('football', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import DefaultTagger\n",
        "\n",
        "default_tagger = DefaultTagger('NN')  # unknown words = noun\n",
        "print(default_tagger.tag([\"University\", \"chatbot\", \"works\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QCSIrjEU8szB",
        "outputId": "902bcfdf-5217-4b54-933d-efd6e5bcacd8"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('University', 'NN'), ('chatbot', 'NN'), ('works', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import UnigramTagger, DefaultTagger\n",
        "from nltk.corpus import brown\n",
        "\n",
        "train = brown.tagged_sents(tagset='universal')[:3000]\n",
        "default = DefaultTagger('NN')\n",
        "tagger = UnigramTagger(train, backoff=default)\n",
        "\n",
        "test = brown.tagged_sents(tagset='universal')[3000:3020]\n",
        "accuracy = tagger.accuracy(test)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lgmk-DWR9PLR",
        "outputId": "3eb018a1-5a47-4adf-a3b8-a670d2b011af"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7796143250688705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.chunking"
      ],
      "metadata": {
        "id": "U0XyEhY8APQZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "text = \"The excellent product quality impressed everyone.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "print(\"POS Tags:\", tags)\n",
        "\n",
        "# Simple pattern for NP: DT/optional + JJ/optional + NN+\n",
        "import re\n",
        "\n",
        "pattern = re.compile(r'(DT\\s)?(JJ\\s)?(NN\\s?)+')\n",
        "tag_str = \" \".join([pos for (word, pos) in tags])\n",
        "print(\"POS String:\", tag_str)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zIf01l50ASKO",
        "outputId": "4617deef-245a-49e3-fc4e-3c72a1ca0378"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "POS Tags: [('The', 'DT'), ('excellent', 'JJ'), ('product', 'NN'), ('quality', 'NN'), ('impressed', 'VBD'), ('everyone', 'NN'), ('.', '.')]\n",
            "POS String: DT JJ NN NN VBD NN .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "# Define NP grammar: DT(optional) + JJ(optional) + NN+\n",
        "grammar = r\"NP: {<DT>?<JJ>*<NN>+}\"\n",
        "\n",
        "chunk_parser = RegexpParser(grammar)\n",
        "chunked = chunk_parser.parse(tags)\n",
        "\n",
        "print(chunked)\n",
        " # optional visual tree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nWmdddOtAaKu",
        "outputId": "e4ecd8de-884e-4e73-ed8b-88bea276e9db"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP The/DT excellent/JJ product/NN quality/NN)\n",
            "  impressed/VBD\n",
            "  (NP everyone/NN)\n",
            "  ./.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. chunker"
      ],
      "metadata": {
        "id": "O_pzRc0TAvtE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.chunk import RegexpParser\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "sentence = \"Apple CEO Tim Cook announced the new iPhone in California.\"\n",
        "tokens = word_tokenize(sentence)\n",
        "tags = pos_tag(tokens)\n",
        "\n",
        "# Define NP (Noun Phrase) and VP (Verb Phrase) grammar\n",
        "grammar = r\"\"\"\n",
        "  NP: {<DT|JJ|NN.*>+}   # Noun phrase\n",
        "  VP: {<VB.*><NP|PP|CLAUSE>+$}  # Verb phrase followed by NP/PP\n",
        "\"\"\"\n",
        "\n",
        "chunker = RegexpParser(grammar)\n",
        "chunked = chunker.parse(tags)\n",
        "\n",
        "print(chunked)\n",
        "  # Optional: visualize the tree\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJmGwy1vAp_L",
        "outputId": "9a7d2e63-8647-4f6f-e9f5-61d6cbb90b12"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (NP Apple/NNP CEO/NNP Tim/NNP Cook/NNP)\n",
            "  announced/VBD\n",
            "  (NP the/DT new/JJ iPhone/NN)\n",
            "  in/IN\n",
            "  (NP California/NNP)\n",
            "  ./.)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.metrics import precision, recall\n",
        "\n",
        "# Example gold-standard NP chunks\n",
        "gold = [('Apple', 'NNP'), ('CEO', 'NNP'), ('Tim', 'NNP'), ('Cook', 'NNP')]\n",
        "pred = [('Apple', 'NNP'), ('CEO', 'NNP'), ('Tim', 'NNP'), ('Cook', 'NNP')]\n",
        "\n",
        "# Convert to sets\n",
        "gold_set = set(gold)\n",
        "pred_set = set(pred)\n",
        "\n",
        "prec = precision(gold_set, pred_set)\n",
        "rec = recall(gold_set, pred_set)\n",
        "\n",
        "print(\"Precision:\", prec)\n",
        "print(\"Recall:\", rec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spPDkE3FA3E9",
        "outputId": "b9945772-cfbf-4497-915c-8052172fc21d"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 1.0\n",
            "Recall: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download resources needed for named entity chunking\n",
        "\n",
        "\n",
        "# Required downloads for NE chunking\n",
        "nltk.download('maxent_ne_chunker_tab')  # This is the one missing\n",
        "nltk.download('words')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "\n",
        "from nltk import ne_chunk\n",
        "\n",
        "# Perform NE chunking on POS-tagged sentence\n",
        "ne_tree = ne_chunk(tags)\n",
        "print(ne_tree)\n",
        "  # Optional visualization\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ury-991rA5b5",
        "outputId": "5aff6609-50eb-4bab-d50a-3682b90955d8"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(S\n",
            "  (PERSON Apple/NNP)\n",
            "  (ORGANIZATION CEO/NNP Tim/NNP Cook/NNP)\n",
            "  announced/VBD\n",
            "  the/DT\n",
            "  new/JJ\n",
            "  iPhone/NN\n",
            "  in/IN\n",
            "  (GPE California/NNP)\n",
            "  ./.)\n"
          ]
        }
      ]
    }
  ]
}