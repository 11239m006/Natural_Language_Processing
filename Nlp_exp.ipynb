{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11239m006/Natural_Language_Processing/blob/main/Nlp_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**\n",
        "\n"
      ],
      "metadata": {
        "id": "5DFBHPfwcxDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "mLLkk8gdawRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "486add5b-ecc1-42dd-976e-48c1932de8cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['NLP is amazing!']\n",
            "Word Tokenization: ['NLP', 'is', 'amazing', '!']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"NLP is amazing!\"\n",
        "print(\"Sentence Tokenization:\", sent_tokenize(text))\n",
        "print(\"Word Tokenization:\", word_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "6AJSeqk1fa7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = \"running\"\n",
        "print(ps.stem(words))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH5dqNWZcGo7",
        "outputId": "b1e656b7-4c15-4ea0-a3e1-4afb8812523b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "t8cscfSNftnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lm = WordNetLemmatizer()\n",
        "print(lm.lemmatize(\"running\", pos='n'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IYtbqQfcceQ",
        "outputId": "62e5408d-69b3-43a0-8594-afb54bc317de"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Morphology (Prefix/Suffix Split Example)**"
      ],
      "metadata": {
        "id": "hg9_iPs2f1kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Text\n",
        "text = \"The cats are playing happily.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "# Print results\n",
        "for token in doc:\n",
        "    print(token.text, \"→\", token.lemma_, \",\", token.pos_, \",\", token.morph)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5GE3mdAcnN6",
        "outputId": "c55246a0-27e8-4a7f-c6ba-45bd5c402380"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The → the , DET , Definite=Def|PronType=Art\n",
            "cats → cat , NOUN , Number=Plur\n",
            "are → be , AUX , Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "playing → play , VERB , Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "happily → happily , ADV , \n",
            ". → . , PUNCT , PunctType=Peri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalization**\n"
      ],
      "metadata": {
        "id": "Jiwkoe6lijN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hey!! What's up??? I'm learning NLP right now...\"\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "# Remove punctuation/special chars\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "# Remove extra spaces\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(\"\\nNormalized Text:\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpb5wZ7cq1X",
        "outputId": "b2f8d35e-7404-4fb8-d957-758c95ddbbdb"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Text: hey whats up im learning nlp right now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **N-Gram (Unigram, Bigram, Trigram)**"
      ],
      "metadata": {
        "id": "xFMg7XoMk3hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing is amazing\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "print(\"\\nUnigrams:\", list(ngrams(tokens, 1)))\n",
        "print(\"Bigrams:\", list(ngrams(tokens, 2)))\n",
        "print(\"Trigrams:\", list(ngrams(tokens, 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYTUfpmLi9qX",
        "outputId": "3c8e2b26-92c5-42b6-c0ec-047bddbe8380"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unigrams: [('natural',), ('language',), ('processing',), ('is',), ('amazing',)]\n",
            "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'amazing')]\n",
            "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'amazing')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **N-Gram Smoothing (Add-1 Laplace)**"
      ],
      "metadata": {
        "id": "brFrpQFclBuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, ngrams\n",
        "from collections import Counter\n",
        "\n",
        "# Sample text\n",
        "text = \"I love NLP. NLP is fun.\"\n",
        "\n",
        "# Tokenize words\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "# Create bigrams (pairs of words)\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "\n",
        "# Count words and bigrams\n",
        "word_counts = Counter(tokens)\n",
        "bigram_counts = Counter(bigrams)\n",
        "\n",
        "# Vocabulary size\n",
        "vocab = len(set(tokens))\n",
        "\n",
        "# Laplace Smoothing function\n",
        "def laplace_prob(w1, w2):\n",
        "    return (bigram_counts[(w1, w2)] + 1) / (word_counts[w1] + vocab)\n",
        "\n",
        "# Example\n",
        "print(\"Laplace Smoothed Probability of ('nlp', 'is'):\", laplace_prob('nlp', 'is'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbSGVOnIlEu9",
        "outputId": "45429ab2-bcfb-46a4-abdf-7d3952de21ff"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Laplace Smoothed Probability of ('nlp', 'is'): 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POS Tagging**"
      ],
      "metadata": {
        "id": "hlTTgGp-lU8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"John is playing football\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\nPOS Tags:\", pos_tag(tokens))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa_7HcodlZ6V",
        "outputId": "24e9dedd-e642-48ca-a67f-8e70e2c5aa73"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tags: [('John', 'NNP'), ('is', 'VBZ'), ('playing', 'VBG'), ('football', 'NN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hidden Markov Model (Simple POS Example using NLTK)**"
      ],
      "metadata": {
        "id": "ZcBOEK2yl9NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "\n",
        "train_data = [[('John', 'NOUN'), ('plays', 'VERB'), ('football', 'NOUN')],\n",
        "              [('She', 'PRON'), ('enjoys', 'VERB'), ('music', 'NOUN')]]\n",
        "\n",
        "tagger = trainer.train_supervised(train_data)\n",
        "print(\"\\nHMM Tagger Result:\", tagger.tag(['She', 'plays', 'football']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QehrK2YvlxMf",
        "outputId": "6aa30b01-a5d9-4e5f-df6b-9ea76fa86c89"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HMM Tagger Result: [('She', 'PRON'), ('plays', 'VERB'), ('football', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Brill/Bidirectional POS Tagger (Bending POS Tagger)**"
      ],
      "metadata": {
        "id": "f9Y71yD6mU7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import brill, brill_trainer\n",
        "\n",
        "train_data = [[('He', 'PRON'), ('is', 'VERB'), ('good', 'ADJ')],\n",
        "              [('She', 'PRON'), ('was', 'VERB'), ('happy', 'ADJ')]]\n",
        "\n",
        "initial_tagger = nltk.DefaultTagger('NOUN')\n",
        "templates = brill.fntbl37()\n",
        "trainer = brill_trainer.BrillTaggerTrainer(initial_tagger, templates)\n",
        "\n",
        "brill_tagger = trainer.train(train_data, max_rules=10)\n",
        "print(\"\\nBrill Tagger Output:\", brill_tagger.tag(['He', 'is', 'happy']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SHHOjlumZxF",
        "outputId": "688c87ae-a6ad-4938-eb64-ecb1784d6106"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Brill Tagger Output: [('He', 'PRON'), ('is', 'VERB'), ('happy', 'ADJ')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **spell correction**"
      ],
      "metadata": {
        "id": "Nv95ifm20aVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "\n",
        "# Input sentence with mistakes\n",
        "text = \"I havv a pencll and a book\"\n",
        "\n",
        "# Create TextBlob object\n",
        "blob = TextBlob(text)\n",
        "\n",
        "# Correct the spelling\n",
        "corrected_text = blob.correct()\n",
        "\n",
        "print(\"Original Text:\", text)\n",
        "print(\"Corrected Text:\", corrected_text)\n"
      ],
      "metadata": {
        "id": "zv3fUCsn0Nl_",
        "outputId": "7eb0ecbb-8d31-4ecb-8cfd-a67c84cf58de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text: I havv a pencll and a book\n",
            "Corrected Text: I have a pencil and a book\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DEDUCTION"
      ],
      "metadata": {
        "id": "HBq5xyXMEuzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True)\n",
        "\n",
        "ps = PorterStemmer()\n",
        "\n",
        "def stem(text):\n",
        "    return [ps.stem(w) for w in word_tokenize(text.lower())]\n",
        "\n",
        "def deduce(p, h):\n",
        "    return \"entailment\" if h in p else \"no entailment\"\n",
        "\n",
        "print(\"Stems:\", stem(\"running runner runs easily fairer\"))\n",
        "print(\"Deduction:\", deduce(\"All men are mortal Socrates is a man\", \"Socrates is mortal\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z2VlGZP8EzkR",
        "outputId": "f4b9d6d2-65e2-4570-ecf7-25f33df72047"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stems: ['run', 'runner', 'run', 'easili', 'fairer']\n",
            "Deduction: no entailment\n"
          ]
        }
      ]
    }
  ]
}