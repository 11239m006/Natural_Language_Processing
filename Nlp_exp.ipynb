{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMANFg1nx00CUmL/fv8jlcZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/11239m006/Natural_Language_Processing/blob/main/Nlp_exp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Tokenization**\n",
        "\n"
      ],
      "metadata": {
        "id": "5DFBHPfwcxDf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mLLkk8gdawRq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbb4912-7d00-494f-9eae-b461743b9513"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization: ['I love Natural Language Processing.', \"It's amazing!\"]\n",
            "Word Tokenization: ['I', 'love', 'Natural', 'Language', 'Processing', '.', 'It', \"'s\", 'amazing', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # ðŸ‘ˆ NEW line â€” required in new NLTK versions\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "text = \"I love Natural Language Processing. It's amazing!\"\n",
        "print(\"Sentence Tokenization:\", sent_tokenize(text))\n",
        "print(\"Word Tokenization:\", word_tokenize(text))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Stemming**"
      ],
      "metadata": {
        "id": "6AJSeqk1fa7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "words = [\"playing\", \"played\", \"plays\"]\n",
        "print([ps.stem(w) for w in words])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KH5dqNWZcGo7",
        "outputId": "2fcd2b14-75c9-4bf8-8c00-434bd21d61c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['play', 'play', 'play']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Lemmatization**"
      ],
      "metadata": {
        "id": "t8cscfSNftnY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lm = WordNetLemmatizer()\n",
        "print(lm.lemmatize(\"running\", pos='v'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_IYtbqQfcceQ",
        "outputId": "81cfa557-1a3a-4c62-dce2-69abb0609488"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "run\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Morphology (Prefix/Suffix Split Example)**"
      ],
      "metadata": {
        "id": "hg9_iPs2f1kN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "text = \"The cats are playing happily.\"\n",
        "doc = nlp(text)\n",
        "\n",
        "print(f\"\\n{'Word':<15} {'Lemma':<15} {'POS':<10} {'Morphology'}\")\n",
        "print(\"-\" * 60)\n",
        "for token in doc:\n",
        "    print(f\"{token.text:<15} {token.lemma_:<15} {token.pos_:<10} {token.morph}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5GE3mdAcnN6",
        "outputId": "80162853-5800-4fc1-aba9-bd2aaeb8400c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word            Lemma           POS        Morphology\n",
            "------------------------------------------------------------\n",
            "The             the             DET        Definite=Def|PronType=Art\n",
            "cats            cat             NOUN       Number=Plur\n",
            "are             be              AUX        Mood=Ind|Tense=Pres|VerbForm=Fin\n",
            "playing         play            VERB       Aspect=Prog|Tense=Pres|VerbForm=Part\n",
            "happily         happily         ADV        \n",
            ".               .               PUNCT      PunctType=Peri\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Normalization**\n"
      ],
      "metadata": {
        "id": "Jiwkoe6lijN-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"Hey!! What's up??? I'm learning NLP right now...\"\n",
        "# Convert to lowercase\n",
        "text = text.lower()\n",
        "# Remove punctuation/special chars\n",
        "text = re.sub(r'[^a-z\\s]', '', text)\n",
        "# Remove extra spaces\n",
        "text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "print(\"\\nNormalized Text:\", text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIpb5wZ7cq1X",
        "outputId": "37957ac8-9d3b-45f7-edb8-17954fba9d2a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized Text: hey whats up im learning nlp right now\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **N-Gram (Unigram, Bigram, Trigram)**"
      ],
      "metadata": {
        "id": "xFMg7XoMk3hr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "text = \"Natural Language Processing is amazing\"\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "print(\"\\nUnigrams:\", list(ngrams(tokens, 1)))\n",
        "print(\"Bigrams:\", list(ngrams(tokens, 2)))\n",
        "print(\"Trigrams:\", list(ngrams(tokens, 3)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NYTUfpmLi9qX",
        "outputId": "aac43e61-4425-4774-9ff3-93a5cce40c0e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unigrams: [('natural',), ('language',), ('processing',), ('is',), ('amazing',)]\n",
            "Bigrams: [('natural', 'language'), ('language', 'processing'), ('processing', 'is'), ('is', 'amazing')]\n",
            "Trigrams: [('natural', 'language', 'processing'), ('language', 'processing', 'is'), ('processing', 'is', 'amazing')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **N-Gram Smoothing (Add-1 Laplace)**"
      ],
      "metadata": {
        "id": "brFrpQFclBuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "corpus = \"I love NLP. NLP is fun.\"\n",
        "tokens = word_tokenize(corpus.lower())\n",
        "bigrams = list(ngrams(tokens, 2))\n",
        "vocab = len(set(tokens))\n",
        "bigram_counts = Counter(bigrams)\n",
        "word_counts = Counter(tokens)\n",
        "\n",
        "def laplace_prob(bigram):\n",
        "    w1, w2 = bigram\n",
        "    return (bigram_counts[bigram] + 1) / (word_counts[w1] + vocab)\n",
        "\n",
        "example_bigram = (\"nlp\", \"is\")\n",
        "print(f\"\\nLaplace Smoothed Probability of {example_bigram}:\", laplace_prob(example_bigram))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FbSGVOnIlEu9",
        "outputId": "a471579b-b730-48fc-b8a2-d069e15b0940"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Laplace Smoothed Probability of ('nlp', 'is'): 0.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **POS Tagging**"
      ],
      "metadata": {
        "id": "hlTTgGp-lU8T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('averaged_perceptron_tagger_eng')  # <-- new required file\n",
        "\n",
        "from nltk import pos_tag\n",
        "\n",
        "text = \"John is playing football\"\n",
        "tokens = word_tokenize(text)\n",
        "print(\"\\nPOS Tags:\", pos_tag(tokens))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xa_7HcodlZ6V",
        "outputId": "ffb10542-174c-4f68-8c6e-d30f67ccdf58"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "POS Tags: [('John', 'NNP'), ('is', 'VBZ'), ('playing', 'VBG'), ('football', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hidden Markov Model (Simple POS Example using NLTK)**"
      ],
      "metadata": {
        "id": "ZcBOEK2yl9NY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import hmm\n",
        "trainer = hmm.HiddenMarkovModelTrainer()\n",
        "\n",
        "train_data = [[('John', 'NOUN'), ('plays', 'VERB'), ('football', 'NOUN')],\n",
        "              [('She', 'PRON'), ('enjoys', 'VERB'), ('music', 'NOUN')]]\n",
        "\n",
        "tagger = trainer.train_supervised(train_data)\n",
        "print(\"\\nHMM Tagger Result:\", tagger.tag(['She', 'plays', 'football']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QehrK2YvlxMf",
        "outputId": "ffa78fa2-4a0f-41e7-c87e-82bd0054d5d1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "HMM Tagger Result: [('She', 'PRON'), ('plays', 'VERB'), ('football', 'NOUN')]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:333: RuntimeWarning: overflow encountered in cast\n",
            "  X[i, j] = self._transitions[si].logprob(self._states[j])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:335: RuntimeWarning: overflow encountered in cast\n",
            "  O[i, k] = self._output_logprob(si, self._symbols[k])\n",
            "/usr/local/lib/python3.12/dist-packages/nltk/tag/hmm.py:331: RuntimeWarning: overflow encountered in cast\n",
            "  P[i] = self._priors.logprob(si)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Brill/Bidirectional POS Tagger (Bending POS Tagger)**"
      ],
      "metadata": {
        "id": "f9Y71yD6mU7b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tag import brill, brill_trainer\n",
        "\n",
        "train_data = [[('He', 'PRON'), ('is', 'VERB'), ('good', 'ADJ')],\n",
        "              [('She', 'PRON'), ('was', 'VERB'), ('happy', 'ADJ')]]\n",
        "\n",
        "initial_tagger = nltk.DefaultTagger('NOUN')\n",
        "templates = brill.fntbl37()\n",
        "trainer = brill_trainer.BrillTaggerTrainer(initial_tagger, templates)\n",
        "\n",
        "brill_tagger = trainer.train(train_data, max_rules=10)\n",
        "print(\"\\nBrill Tagger Output:\", brill_tagger.tag(['He', 'is', 'happy']))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1SHHOjlumZxF",
        "outputId": "ae81c041-573b-4220-a33d-fe019b2a2859"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Brill Tagger Output: [('He', 'PRON'), ('is', 'VERB'), ('happy', 'ADJ')]\n"
          ]
        }
      ]
    }
  ]
}